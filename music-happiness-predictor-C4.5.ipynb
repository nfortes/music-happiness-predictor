{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31908c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course : CS 513 - Knowledge Discovery and Data Mining\n",
    "# Group Members: Branden Bulatao, Joseph Faustino, Natalie Fortes, Isabel Sutedjo\n",
    "# Id : 20005971, 20006114, 20006007, 20006618\n",
    "# Purpose : Music Happiness Predictor - Predicts the happiness of music tracks based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09597393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5-style Decision Tree Accuracy: 53.17 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Filter for simplicity - top 5 most common genres\n",
    "top_genres = df['track_genre'].value_counts().nlargest(5).index\n",
    "df = df[df['track_genre'].isin(top_genres)]\n",
    "\n",
    "# Select relevant features\n",
    "features = ['danceability', 'energy', 'acousticness', 'instrumentalness', 'valence', 'tempo']\n",
    "X = df[features]\n",
    "y = df['track_genre']\n",
    "\n",
    "# Discretize continuous features (e.g. using quartiles)\n",
    "for col in X.columns:\n",
    "    X.loc[:, col] = pd.qcut(X[col], q=4, labels=False, duplicates='drop')\n",
    "\n",
    "# Combine for easier handling\n",
    "df = X.copy()\n",
    "df['label'] = y\n",
    "\n",
    "# --- C4.5 Functions ---\n",
    "def entropy(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    return -sum((count/total) * math.log2(count/total) for count in counts.values())\n",
    "\n",
    "def info_gain_ratio(df, attr, target):\n",
    "    total_entropy = entropy(df[target])\n",
    "    values = df[attr].unique()\n",
    "    splits = []\n",
    "    split_info = 0\n",
    "    weighted_entropy = 0\n",
    "\n",
    "    for val in values:\n",
    "        subset = df[df[attr] == val]\n",
    "        prob = len(subset) / len(df)\n",
    "        weighted_entropy += prob * entropy(subset[target])\n",
    "        split_info -= prob * math.log2(prob) if prob > 0 else 0\n",
    "\n",
    "    gain = total_entropy - weighted_entropy\n",
    "    return gain / split_info if split_info != 0 else 0\n",
    "\n",
    "def build_tree(df, target, features):\n",
    "    labels = df[target]\n",
    "    if len(set(labels)) == 1:\n",
    "        return labels.iloc[0]\n",
    "    if len(features) == 0:\n",
    "        return labels.mode()[0]\n",
    "\n",
    "    gains = {feature: info_gain_ratio(df, feature, target) for feature in features}\n",
    "    best_feature = max(gains, key=gains.get)\n",
    "    tree = {best_feature: {}}\n",
    "\n",
    "    for val in df[best_feature].unique():\n",
    "        subset = df[df[best_feature] == val]\n",
    "        subtree = build_tree(subset, target, [f for f in features if f != best_feature])\n",
    "        tree[best_feature][val] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "def predict(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    attr = next(iter(tree))\n",
    "    val = sample[attr]\n",
    "    subtree = tree[attr].get(val)\n",
    "    if subtree is None:\n",
    "        return None\n",
    "    return predict(subtree, sample)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df['label'], test_size=0.2, random_state=42)\n",
    "train_df = X_train.copy()\n",
    "train_df['label'] = y_train\n",
    "\n",
    "# Train tree\n",
    "decision_tree = build_tree(train_df, 'label', features)\n",
    "\n",
    "# Predict\n",
    "y_pred = [predict(decision_tree, row) for _, row in X_test.iterrows()]\n",
    "\n",
    "# Remove Nones from unknowns\n",
    "valid_idx = [i for i, val in enumerate(y_pred) if val is not None]\n",
    "accuracy = accuracy_score(y_test.iloc[valid_idx], [y_pred[i] for i in valid_idx])\n",
    "print(\"C4.5-style Decision Tree Accuracy:\", round(accuracy * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea42a7-61d6-42c3-98c1-a5dcbb94ec9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
