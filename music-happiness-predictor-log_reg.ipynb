{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course : CS 513 - Knowledge Discovery and Data Mining\n",
    "# Group Members: Branden Bulatao, Joseph Faustino, Natalie Fortes, Isabel Sutedjo\n",
    "# Id : 20005971\n",
    "# Purpose : Music Happiness Predictor - Predicts the happiness of music tracks based on various features.\n",
    "\n",
    "# Main Author: Branden Bulatao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clips outliers Q1–1.5×IQR and Q3+1.5×IQR\n",
    "def clip_outliers(df, feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Clip the values\n",
    "    df[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    # sns.boxplot(x=f\"{feature}\", data=df)\n",
    "    # plt.title(f\"{feature} by Valence Group\")\n",
    "    # plt.show()\n",
    "    return df\n",
    "\n",
    "def clip_outliers_strict(df, feature):\n",
    "    lower = df[feature].quantile(0.005)\n",
    "    upper = df[feature].quantile(0.995)\n",
    "    df[feature] = df[feature].clip(lower=lower, upper=upper)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Check Outlier percentages of each feature\n",
    "def calculate_outlier_percentage(df, feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    outlier_percentage = len(outliers) / len(df) * 100\n",
    "    return outlier_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv(\"./dataset.csv\")\n",
    "\n",
    "df.drop(\n",
    "    df.columns[df.columns.str.contains(\"unnamed\", case=False)], axis=1, inplace=True\n",
    ")  # drop unnamed column\n",
    "\n",
    "df.dropna(inplace=True)  # drop rows with null values\n",
    "\n",
    "df[\"valence\"] = df[\"valence\"].astype(\"category\")\n",
    "df[\"valence\"] = df[\"valence\"].map(\n",
    "    lambda x: 0 if x < 0.5 else 1\n",
    ")  # sets valence to 0 if sad, 1 if happy\n",
    "\n",
    "# Convert 'explicit' boolean to integer (0/1)\n",
    "df[\"explicit\"] = df[\"explicit\"].astype(int)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df = pd.get_dummies(\n",
    "    df, columns=[\"key\", \"time_signature\", \"track_genre\"], drop_first=True\n",
    ")\n",
    "\n",
    "# X = df.drop(['track_id', 'artists', 'album_name', 'track_name', 'valence'], axis=1)\n",
    "X = df.drop([\"track_id\", \"artists\", \"album_name\", \"track_name\", \"valence\", \"duration_ms\"], axis=1)\n",
    "y = df[\"valence\"]\n",
    "\n",
    "# Identify numeric features to scale\n",
    "numeric_features = [\n",
    "    # \"duration_ms\",\n",
    "    # \"popularity\",\n",
    "    # \"tempo\",\n",
    "    # \"loudness\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "]\n",
    "\n",
    "numeric_high_value_features = [\n",
    "    \"popularity\",\n",
    "    \"tempo\",\n",
    "    \"loudness\",\n",
    "]\n",
    "\n",
    "# All other features (binary or one-hot) are left as-is\n",
    "non_scaled_features = [\n",
    "    col \n",
    "    for col in X.columns \n",
    "    if col not in (numeric_features + numeric_high_value_features)\n",
    "]\n",
    "# non_scaled_features = [col for col in X.columns if col not in numeric_high_value_features]\n",
    "\n",
    "# Clip extreme outliers >5%\n",
    "# threshold = 5\n",
    "# for col in numeric_features + numeric_high_value_features:\n",
    "#     percent = calculate_outlier_percentage(X, col)\n",
    "#     if percent > threshold:\n",
    "#         X = clip_outliers_strict(X, col)\n",
    "#         print(f\"Clipped {col} (outliers were {percent:.2f}%)\")\n",
    "\n",
    "# ColumnTransformer for selective scaling\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"num2\", MinMaxScaler(), numeric_high_value_features),\n",
    "        (\"pass\", \"passthrough\", non_scaled_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Reconstruct a DataFrame (optional, for inspection/debugging)\n",
    "final_features = numeric_features + numeric_high_value_features + non_scaled_features\n",
    "X_processed = pd.DataFrame(X_processed, columns=final_features)\n",
    "# X_processed = X_processed.apply(pd.to_numeric)  # <-- this line fixes your problem\n",
    "\n",
    "# Train the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7727485380116959 \n",
    "0.7721637426900585 \n",
    "0.772719298245614 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred), \"\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\", cmap=\"Blues\", ax=ax)\n",
    "\n",
    "# Labels, title, and ticks\n",
    "ax.set_xlabel(\"Predicted Labels\")\n",
    "ax.set_ylabel(\"True Labels\")\n",
    "ax.set_title(\"KNN Confusion Matrix for Valence\")\n",
    "ax.xaxis.set_ticklabels([\"Sad (0)\", \"Happy (1)\"])\n",
    "ax.yaxis.set_ticklabels([\"Sad (0)\", \"Happy (1)\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict_proba(X_test)\n",
    "# y_log_proba = model.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"Model (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Random guess line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "coeffs = pd.Series(\n",
    "    model.coef_[0],\n",
    "    index= numeric_features + numeric_high_value_features + non_scaled_features,\n",
    ")\n",
    "print(coeffs.sort_values(ascending=False))\n",
    "\n",
    "coeffs.sort_values().plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "\n",
    "# Get top 10 positive and negative\n",
    "top_features = pd.concat([coeffs.nlargest(10), coeffs.nsmallest(10)])\n",
    "\n",
    "# Plot them\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features.sort_values().plot(\n",
    "    kind=\"barh\", color=[\"red\" if v < 0 else \"green\" for v in top_features.sort_values()]\n",
    ")\n",
    "plt.title(\"Top Positive & Negative Features\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numeric_features + numeric_high_value_features + non_scaled_features\n",
    "print(pd.Series(features).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove genre columns from the coefficients\n",
    "print(non_scaled_features)\n",
    "non_genre_features = list(set(numeric_features + numeric_high_value_features))\n",
    "non_genre_features = [f for f in non_genre_features if f in coeffs.index]\n",
    "non_genre_coeffs = coeffs[non_genre_features]\n",
    "\n",
    "# non_genre_coeffs = coeffs[numeric_features]\n",
    "# non_genre_coeffs = coeffs[numeric_features] + coeffs[numeric_high_value_features]\n",
    "\n",
    "# Get top 10 and bottom 10\n",
    "top_non_genre = non_genre_coeffs.nlargest(10)\n",
    "\n",
    "print(top_non_genre)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_non_genre.sort_values().plot(\n",
    "    kind=\"barh\",\n",
    "    color=[\"red\" if v < 0 else \"green\" for v in top_non_genre.sort_values()],\n",
    ")\n",
    "plt.title(\"Top Positive & Negative Non-Genre Features\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = np.max(y_proba, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
